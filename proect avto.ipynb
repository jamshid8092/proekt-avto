{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf8'\n",
    "    return BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brand_links():\n",
    "    url = f'https://auto.ru'\n",
    "    soup_main = get_page(url)\n",
    "\n",
    "    all_brand_links = []\n",
    "\n",
    "    for brand in soup_main.find_all('a', class_='IndexMarks__item'):\n",
    "        all_brand_links.append(brand.get('href').replace('all', 'used'))\n",
    "\n",
    "    return all_brand_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_number(url):\n",
    "    strr = 'ButtonWithLoader__content'\n",
    "    page = get_page(url)\n",
    "    cntt = page.find(class_=strr).text.split(' ')[1].split('\\xa0')\n",
    "    if len(cntt[1]) > 3:\n",
    "        cnt = int(cntt[0])\n",
    "    else:\n",
    "        cnt = int(cntt[0]+cntt[1])\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_car_links(page_url):\n",
    "    page = get_page(page_url)\n",
    "    if page == None:\n",
    "        return []\n",
    "    all_link = page.find_all(class_='ListingItemTitle-module__link')\n",
    "    links = []\n",
    "    for link in all_link:\n",
    "        links.append(link['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_car_links(url):\n",
    "    try:\n",
    "        strr = 'ListingPagination-module__page'\n",
    "        max_page_num = int(get_page(url).find_all(class_=strr)[-1].text)\n",
    "    except:\n",
    "        max_page_num = 0\n",
    "\n",
    "    links = []\n",
    "\n",
    "    page_urls = [url + '?page=' + str(i) for i in range(1, max_page_num+1)]\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for url in page_urls:\n",
    "            futures.append(executor.submit(get_page_car_links, page_url=url))\n",
    "        for feature in concurrent.futures.as_completed(futures):\n",
    "            links += feature.result()\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(brand_links):\n",
    "    car_body = ['sedan', 'hatchback', 'allroad', 'wagon',\n",
    "                'coupe', 'minivan', 'pickup', 'limousine', 'van', 'cabrio']\n",
    "    car_links = []\n",
    "    for link in brand_links:\n",
    "        cnt = check_number(link)\n",
    "        if cnt <= 3700:\n",
    "            car_links += get_car_links(link)\n",
    "        else:\n",
    "            for year in range(1999, 2022):\n",
    "                if year > 2000:\n",
    "                    aa = link.split('/')\n",
    "                    url_1 = '/'.join(aa[:5])+'/'+str(year)+'-year/used/'\n",
    "                else:\n",
    "                    url_1 = link+'?year_to=2000'\n",
    "                cnt = check_number(url_1)\n",
    "                if cnt <= 3700:\n",
    "                    car_links += get_car_links(url_1)\n",
    "                else:\n",
    "                    for cb in car_body:\n",
    "                        if year > 2000:\n",
    "                            url_2 = url_1+f'body-{cb}/'\n",
    "                        else:\n",
    "                            url_2 = link+f'body-{cb}/?year_to=2000'\n",
    "                        car_links += get_car_links(url_2)\n",
    "    return car_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_car_info(car_link):\n",
    "    soup_car = get_page(car_link)\n",
    "    car_info = {}\n",
    "    cls_str = 'CardInfoRow_'\n",
    "    span_str = 'CardInfoRow__cell'\n",
    "    regex = re.compile('.*__info-item.*')\n",
    "    sd = 'sale-data-attributes'\n",
    "    \n",
    "    if soup_car.find('div', class_='CardSold') == None:\n",
    "\n",
    "        car_info['datetime'] = datetime.datetime.now(\n",
    "            datetime.timezone(datetime.timedelta(hours=3)))\n",
    "        car_info['parsing_unixtime'] = int(time.time())\n",
    "        car_info['card'] = soup_car.find(\n",
    "            'div', class_='CardSidebarActions__title').text\n",
    "        car_info['region'] = soup_car.find(\n",
    "            'span', class_='MetroListPlace__regionName MetroListPlace_nbsp').text\n",
    "        if soup_car.find('div', class_='CardSellerNamePlace__name'):\n",
    "            car_info['sellerName'] = soup_car.find(\n",
    "                'div', class_='CardSellerNamePlace__name').text\n",
    "        elif soup_car.find('a', class_='CardSellerNamePlace__name_dealer'):\n",
    "            car_info['dealerName'] = soup_car.find(\n",
    "                'a', class_='CardSellerNamePlace__name_dealer').text\n",
    "        car_info['sell_id'] = soup_car.find(\n",
    "            'div', title='Идентификатор объявления').text\n",
    "        car_info['car_url'] = car_link\n",
    "        car_info['price'] = soup_car.find(\n",
    "            'span', class_='OfferPriceCaption__price').text\n",
    "        car_info['description'] = soup_car.find(\n",
    "            'div', class_='CardDescription__textInner').text\n",
    "        car_info['image'] = 'https:' + \\\n",
    "            soup_car.find(\n",
    "                'img', class_='ImageGalleryDesktop__image').get('src')\n",
    "\n",
    "        soup_name = soup_car.find(\n",
    "            'div', class_='CardBreadcrumbs').find_all('a')\n",
    "\n",
    "        car_info['bodyType'] = soup_car.find(\n",
    "            'li', class_=cls_str+'bodytype').find('a').text\n",
    "        car_info['color'] = soup_car.find(\n",
    "            'li', class_=cls_str+'color').find('a').text\n",
    "        car_info['engine'] = soup_car.find(\n",
    "            'li', class_=cls_str+'engine').find('div').text\n",
    "        car_info['engineDisplacement'], car_info['enginePower'], car_info['fuelType'] = car_info['engine'].split(\n",
    "            '/')\n",
    "        car_info['mileage'] = soup_car.find(\n",
    "            'li', class_=cls_str+'kmAge').find_all('span')[1].text\n",
    "        car_info['productionDate'] = soup_car.find(\n",
    "            'li', class_=cls_str+'year').find_all('span')[1].text\n",
    "        car_info['vehicleTransmission'] = soup_car.find(\n",
    "            'li', class_=cls_str+'transmission').find_all('span')[1].text\n",
    "        car_info['Владельцы'] = soup_car.find(\n",
    "            'li', class_=cls_str+'ownersCount').find_all('span')[1].text\n",
    "        car_info['ПТС'] = soup_car.find(\n",
    "            'li', class_=cls_str+'pts').find_all('span')[1].text\n",
    "        car_info['Привод'] = soup_car.find(\n",
    "            'li', class_=cls_str+'drive').find_all('span')[1].text\n",
    "        car_info['Руль'] = soup_car.find(\n",
    "            'li', class_=cls_str+'wheel').find_all('span')[1].text\n",
    "        car_info['Состояние'] = soup_car.find(\n",
    "            'li', class_=cls_str+'state').find_all('span')[1].text\n",
    "        car_info['Таможня'] = soup_car.find(\n",
    "            'li', class_=cls_str+'customs').find_all('span')[1].text\n",
    "        car_info['card_type'] = soup_name[1].text\n",
    "        car_info['brand'] = soup_name[2].text\n",
    "        car_info['model_name'] = soup_name[3].text\n",
    "        car_info['model_name2'] = soup_name[4].text\n",
    "        car_info['name'] = soup_name[6].text\n",
    "        car_info['equipment_dict'] = sale_data = json.loads(\n",
    "            soup_car.find(id=sd)['data-bem'])[sd]\n",
    "\n",
    "        car_info['public_date'] = soup_car.find_all(\n",
    "            'div', {\"class\": regex})[0].text\n",
    "        car_info['nview'] = soup_car.find_all('div', {\"class\": regex})[\n",
    "            1].text.split(' ')[0]\n",
    "        car_info['model_name_full'] = soup_car.find('h1').text\n",
    "        url_2 = soup_car.find('a', class_='SpoilerLink')['href']\n",
    "        pag = get_page(url_2)\n",
    "        car_info['confDict'] = json.loads(pag.find(id=sd)['data-bem'])[sd]\n",
    "    else:\n",
    "        car_info = {}\n",
    "\n",
    "    return car_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(urls):\n",
    "    data = []\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        future_to_url = {executor.submit(\n",
    "            get_car_info, url): url for url in urls}\n",
    "        for future in concurrent.futures.as_completed(future_to_url):\n",
    "            url = future_to_url[future]\n",
    "            try:\n",
    "                data.append(future.result())\n",
    "            except Exception as exc:\n",
    "                print('%r generated an exception: %s' % (url, exc))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://auto.ru/cars/vaz/used/ 65250\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'res_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7ac384e8f9e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrand\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m             \u001b[0mres_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrand\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res_dict' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-7ac384e8f9e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mres_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrand\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_links\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mres_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbrand\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Load time:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mthreaded_start\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'res_dict' is not defined"
     ]
    }
   ],
   "source": [
    "for link in br:\n",
    "    print(link, check_number(link))\n",
    "    threaded_start = time.time()\n",
    "    brand = link.split('/')[4]\n",
    "    try:\n",
    "        if len(res_dict[brand])==0:\n",
    "            res_dict[brand] = get_links([link])\n",
    "    except:\n",
    "        res_dict[brand]=[]\n",
    "    print(\"Load time:\", time.time() - threaded_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
